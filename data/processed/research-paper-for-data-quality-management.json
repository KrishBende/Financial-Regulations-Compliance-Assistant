[
  {
    "chunk_id": 0,
    "text": "Available online at www.sciencedirect.com\nScienceDirect\nProcedia Computer Science 256 (2025) 292–299\nCENTERIS – International Conference on ENTERprise Information Systems / ProjMAN –\nInternational Conference on Project MANagement / HCist – International Conference on\nHealth and Social Care Information Systems and Technologies 2024\nApplying Markov Chains in Data Quality Management for GDPR\nCompliance: A New Perspective\nAntónio Gonçalves1,2*, Anacleto Correia1\n1CINAV, Almada, 2810-001, Portugal\n2INESC-ID, Lisboa, 1000-029, Portugal\nAbstract\nIn today's digital context, data quality management is of critical importance, especially consideringthe General Data Protection"
  },
  {
    "chunk_id": 1,
    "text": "2INESC-ID, Lisboa, 1000-029, Portugal\nAbstract\nIn today's digital context, data quality management is of critical importance, especially consideringthe General Data Protection\nRegulation (GDPR). This regulation imposes the need for accurate and up-to-date data, underlining strict data management to\nprotect the privacy and security off personal data. Markov Chains represent a promising predictive approach, offering a tool for\norganizations to anticipate and mitigate data security risks while improving GDPR compliance. This study highlights the\nfundamental connection between data quality and breach prevention, proposing the use of Markov Chains as an innovative"
  },
  {
    "chunk_id": 2,
    "text": "fundamental connection between data quality and breach prevention, proposing the use of Markov Chains as an innovative\nmethod to improve data quality management, regulatory compliance, and personal data security, suggesting a proactive direction\nfor future organizational practices.\n© 2025 The Authors. Published by Elsevier B.V.\nThis is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0)\nPeer-review under responsibility of the scientific committee of the CENTERIS - International Conference on ENTERprise Information"
  },
  {
    "chunk_id": 3,
    "text": "Peer-review under responsibility of the scientific committee of the CENTERIS - International Conference on ENTERprise Information\nSystems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care\nInformation Systems and Technologies\nKeywords:GDPR; data quality; markov chain; compliance; data breach\n* Corresponding author. Tel.: +0-000-000-0000 ; fax: +0-000-000-0000 .\nE-mail address:agoncalveslx@gmail.com\n1877-0509 © 2025 The Authors. Published by Elsevier B.V.\nThis is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0)"
  },
  {
    "chunk_id": 4,
    "text": "1877-0509 © 2025 The Authors. Published by Elsevier B.V.\nThis is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0)\nPeer-review under responsibility of the scientific committee of the CENTERIS - International Conference on ENTERprise\nInformation Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference\non Health and Social Care Information Systems and Technologies\n10.1016/j.procs.2025.02.123\nAntónio Gonçalves et al. / Procedia Computer Science 256 (2025) 292–299 293\n1.introduction\nAt the forefront of the digital era, the transformative shift within the domain of information systems underscores"
  },
  {
    "chunk_id": 5,
    "text": "1.introduction\nAt the forefront of the digital era, the transformative shift within the domain of information systems underscores\nthe paramount importance of data quality as a foundational pillar for the successful operation of entities across various\nsectors. The advent of the General Data Protection Regulation (GDPR) has significantly amplified the complexities\ninherent in data stewardship, necessitating an unwavering compliance with stringent regulatory frameworks designed\nto safeguard the privacy and integrity of personal data. This evolving paradigm mandates organizations to maintain\nrigorous standards of data accuracy, timeliness, and security from inception to ultimate application."
  },
  {
    "chunk_id": 6,
    "text": "rigorous standards of data accuracy, timeliness, and security from inception to ultimate application.\nThe integration of Markov Chains into this framework represents a significant paradigm shift, offering a structured\nand predictive methodology by which organizations can proactively identify and address potential vulnerabilities\nwithin their data security mechanisms, thereby elevating the valueof data throughout its lifecycle.\nAmid the heightened value on data quality engendered by GDPR compliance, and the promising potential of\nMarkov Chains in upholding such quality, this article probes the pivotal research question:"
  },
  {
    "chunk_id": 7,
    "text": "Amid the heightened value on data quality engendered by GDPR compliance, and the promising potential of\nMarkov Chains in upholding such quality, this article probes the pivotal research question:\n\"How does the application of Markov Chains in data quality management enhance GDPR compliance and diminish\nthe risk of personal data breaches within organizational infrastructures?\"\nThis investigation endeavours to integrate the theoretical underpinnings of Markov Chains with their practical\napplications in data quality management, presenting an innovative perspective on the utilization of predictive analytics\nto bolster data security and ensure compliance with regulatory mandates."
  },
  {
    "chunk_id": 8,
    "text": "applications in data quality management, presenting an innovative perspective on the utilization of predictive analytics\nto bolster data security and ensure compliance with regulatory mandates.\nThe article progresses with a detailed exposition on the intrinsic value of data quality within the GDPR framework,\nsubsequently transitioning to an exploration of contemporary data management practices. An in-depth analysis of\nMarkov Chains and their theoretical application to enhancing data security sets the stage for the innovative proposition\ndelineated, which posits a predictive model for the anticipation of data breaches. The subsequent sections delve into"
  },
  {
    "chunk_id": 9,
    "text": "delineated, which posits a predictive model for the anticipation of data breaches. The subsequent sections delve into\nthe ethical and practical considerations of the proposed model, evaluating its implications in the broader context of\nGDPR compliance. The concluding section aggregates the study's findings, outlining future research avenues and best\npractices designed to elevate GDPR adherence.\n2.Data quality under the GDPR\nThe concept of data quality is dynamic, constantly evolving to meet the emerging demands and challenges in\ninformation management. Echoing the pioneering views of Brodie [1], data quality is heralded as a vital element across"
  },
  {
    "chunk_id": 10,
    "text": "information management. Echoing the pioneering views of Brodie [1], data quality is heralded as a vital element across\nthe data lifecycle, transcending basic collection and storage to include ongoing maintenance and assessment through\nthe integration of novel tools and cutting-edge concepts and methodologies. This holistic view ensures data remains\naccurate, trustworthy, and relevant to user needs.\nData quality's role is pivotal, influencing the efficacy of information systems, bolstering user confidence, and\nensuring regulatory compliance. As data becomes increasingly fundamental to business operations and decision-"
  },
  {
    "chunk_id": 11,
    "text": "ensuring regulatory compliance. As data becomes increasingly fundamental to business operations and decision-\nmaking, the imperative for maintaining high-quality standards intensifies. Notably, data quality is essential for\nadhering to the General Data Protection Regulation (GDPR), which mandates the accuracy and, when necessary, the\ntimeliness of personal data updates. Consequently, data quality management transitions from a best practice to a\nregulatory mandate across various contexts, highlighting its critical importance for organizational functionality.\nHence, GDPR elevates data quality as a key standard for data privacy and security protection, compelling organizations"
  },
  {
    "chunk_id": 12,
    "text": "Hence, GDPR elevates data quality as a key standard for data privacy and security protection, compelling organizations\nto ensure the processed data's precision, currency, and retention only as required for specified purposes [2].\nAs outlined by Balau [3], GDPR necessitates strict adherence to data quality standards by data controllers,\nemphasizing accuracy, relevance, and data minimization. The regulation significantly influences how organizations\ngather, store, and utilize personal data. Menges [4]elaborates on the significance of pseudonymization in bolstering\ndata security under GDPR, showcasing the interplay between data quality and privacy protection strategies in"
  },
  {
    "chunk_id": 13,
    "text": "data security under GDPR, showcasing the interplay between data quality and privacy protection strategies in\nmitigating security threats.\nIn conclusion, data quality is a multifaceted discipline that lies at the heart of digital transformation. It is a critical\nsuccess factor for organizations aiming to thrive in the digital age, requiring a holistic approach that integrates\ntechnology, processes, and people. The future of digital innovation and governance will increasingly depend on the\nability of organizations to maintain high standards of data quality, underscoring its importance as a strategic asset in\nthe quest for excellence, security, and compliance in the digital domain."
  },
  {
    "chunk_id": 14,
    "text": "ability of organizations to maintain high standards of data quality, underscoring its importance as a strategic asset in\nthe quest for excellence, security, and compliance in the digital domain.\n294 António Gonçalves et al. / Procedia Computer Science 256 (2025) 292–299\n3.Markovchain and data quality\nMarkov Chains [5], named after the Russian mathematician Andrey Markov, are a class of stochastic processes that\nhave found extensive applications in various fields, from probability theory to computer science and engineering. The\nessence of these processes lies in the \"memoryless\" property, or Markov property, where the probability of"
  },
  {
    "chunk_id": 15,
    "text": "essence of these processes lies in the \"memoryless\" property, or Markov property, where the probability of\ntransitioning to the next state depends only on the current state and not on the sequence of events that preceded it.\nMarkov Chains are models used to describe systems that undergo transitions between states with defined\nprobabilities, independent of previous history (the \"memoryless\" property). The \"states\" represent the different\npossible conditions of the system, while the \"transition probabilities\" indicate the chance of moving from one state to\nanother, all summarized in a \"transition matrix.\" Each row of this matrix must sum to 1, representing all possible next"
  },
  {
    "chunk_id": 16,
    "text": "another, all summarized in a \"transition matrix.\" Each row of this matrix must sum to 1, representing all possible next\nconditions. The \"time step\" refers to the interval between transitions. The \"initial state distribution\" defines the initial\nprobabilities of each state, and the \"steady-state distribution\" describes the probabilities that the system reaches after\nan extensive number of steps, reflecting the long-term behaviourof the system [5].\nThe application of Markov Chains in data quality management highlights a systematic and predictive approach,\nessential for the development of robust, accurate, and efficient information systems. By utilizing Markov Chains,"
  },
  {
    "chunk_id": 17,
    "text": "essential for the development of robust, accurate, and efficient information systems. By utilizing Markov Chains,\norganizations gain a detailed understanding of data quality dynamics, essential for making decisions that seek\naccuracy, reliability, and efficiency in data systems.\n4.Data quality and personal data breach under the GDPR\nThe GDPR represents a significant stride forward in the field of data protection, setting forth comprehensive\nrequirements for the collection, storage, and management of personal data. This regulatory framework ensures that\ndata is processed in a manner that is fair, transparent, and secure. The introduction of GDPR has underscored the"
  },
  {
    "chunk_id": 18,
    "text": "data is processed in a manner that is fair, transparent, and secure. The introduction of GDPR has underscored the\nimportance of data integrity and the rights of individuals, thereby reshaping the landscape of personal data\nmanagement across the European Union and beyond[6].This set of precepts are an integral part of the GDPR through\na set of articles and recitals presented in Table I.\nTable 1. GDPR and data quality.\nArticle Recitals Description\n5 39, 58 It defines essential principles for the processing of personal data, emphasizing the\naccuracy and timeliness of the data.\n6 39,58 Establishes the legal conditions for processing personal data, ensuring that they are"
  },
  {
    "chunk_id": 19,
    "text": "accuracy and timeliness of the data.\n6 39,58 Establishes the legal conditions for processing personal data, ensuring that they are\npertinent and restricted to what is essential for their purposes.\n15 66 Defines the right of individuals to verify and correct their personal data processed\nby the controller.\n16 66 It guarantees the right of data subjects to rectify inaccurate or incomplete personal\ndata concerning them.\n23 39, 58 It allows member states to establish exceptions to the principle of accuracy of\npersonal data to ensure the protection of the freedom and rights of data subjects.\n25 78 Discusses the initial integration of data protection measures, such as"
  },
  {
    "chunk_id": 20,
    "text": "personal data to ensure the protection of the freedom and rights of data subjects.\n25 78 Discusses the initial integration of data protection measures, such as\npseudonymization and data minimization, into systems and products to improve\ndata quality.\n32 39 It stipulates that the controller and the controller must implement appropriate\ntechnical and organisational measures to ensure a level of security appropriate to\nthe risk, including the accuracy of personal data.\nAntónio Gonçalves et al. / Procedia Computer Science 256 (2025) 292–299 295\nThis table summarizes the key components of the GDPR that are related to data quality, reflecting how these"
  },
  {
    "chunk_id": 21,
    "text": "António Gonçalves et al. / Procedia Computer Science 256 (2025) 292–299 295\nThis table summarizes the key components of the GDPR that are related to data quality, reflecting how these\nprinciples and legal requirements interconnect to promote accuracy, security, and adequacy in the processing of\npersonal data. Data quality is not just a matter of compliance but also an ethical and practical issue, essential for the\ntrust of data subjects and for the effectiveness of data processing operations.\nUnder the GDPR, data quality is an integral part of data protection from the outset (data protection by design) and"
  },
  {
    "chunk_id": 22,
    "text": "trust of data subjects and for the effectiveness of data processing operations.\nUnder the GDPR, data quality is an integral part of data protection from the outset (data protection by design) and\nby default (data protection by default), meaning that appropriate measures must be implemented to ensure that\npersonal data is processed to the highest standard of security and only for the necessary period. Furthermore, data\nsubjects have the right to rectify incorrect or incomplete data[7].\nA personal data breach occurs when there is a security failure that results, unintentionally or unlawfully, in the"
  },
  {
    "chunk_id": 23,
    "text": "subjects have the right to rectify incorrect or incomplete data[7].\nA personal data breach occurs when there is a security failure that results, unintentionally or unlawfully, in the\ndestruction, loss, alteration, unauthorized disclosure of, or access to, personal data transmitted, stored, or otherwise\nprocessed. This incident can affect the confidentiality, integrity, or availability of personal data and results in various\nrisks to the rights and freedoms of individuals, including financial losses, damage to reputation, loss of confidentiality\nof data protected by professional secrecy, discrimination, and other significant disadvantages."
  },
  {
    "chunk_id": 24,
    "text": "of data protected by professional secrecy, discrimination, and other significant disadvantages.\nThe lack of data quality, by contributing to the inaccuracy, incompleteness, or outdatedness of personal data,\nsignificantly increases the risk of personal data breaches. Poor quality data can lead to inadequate management and\nincorrect application of security measures, increasing the chances of a breach and making the detection and response\nprocess more difficult. Thus, ensuring data quality is fundamental to reducing the possibility of personal data breaches\nand to ensuring compliance with data protectionregulations such as the GDPR.The relationship between data quality"
  },
  {
    "chunk_id": 25,
    "text": "and to ensuring compliance with data protectionregulations such as the GDPR.The relationship between data quality\nand personal data breaches is fundamentally as follows [8]: Breach Prevention: The prevention of personal data\nbreaches starts with maintaining high data quality, a premise based on the triad of accuracy, relevance, and security.\nEach of these elements plays a crucial role in mitigating risks associated with data management. Breach Impact:\nData quality also affects the severity of a personal data breach. Inaccurate or outdated data can amplify the negative\nimpact of a breach, affecting decisions based on incorrect information or exposing information of individuals who"
  },
  {
    "chunk_id": 26,
    "text": "impact of a breach, affecting decisions based on incorrect information or exposing information of individuals who\nshould not be in the database. Accountability and Legal Consequences: The adoption of the GDPR sets strict\nstandards for the management and security of this information. One of the fundamental requirements of the GDPR is\nthe obligation of organizations to ensure data quality, emphasizing the accuracy, relevance, and timeliness of the\npersonal information they process. Failure to maintain data quality is not just an internal management issue; under the\nGDPR, such negligence is interpreted as a breach of legal obligations. Breach Response:Data quality is crucial for"
  },
  {
    "chunk_id": 27,
    "text": "GDPR, such negligence is interpreted as a breach of legal obligations. Breach Response:Data quality is crucial for\neffectively managing a data breach, serving as the foundation for response and recovery processes. Accurate and up-\nto-date data allow organizations to quickly identify which information was affected and which individuals need to be\nnotified. Transparency and Trust: Maintaining data quality is essential not only for compliance but also for\nestablishing trust between organizations and individuals. Transparency in data management is crucial to foster this\ntrust. Rigorous practices that ensure data accuracy, timeliness, and relevance demonstrate a commitment to integrity"
  },
  {
    "chunk_id": 28,
    "text": "trust. Rigorous practices that ensure data accuracy, timeliness, and relevance demonstrate a commitment to integrity\nand privacy.Fulfilment of Data Subject Rights:Data quality is directly related to an organization's ability to fulfil\nthe rights of data subjects under the GDPR, such as the right to access, rectification, and erasure. Poor data quality\nposes a significant threat not only to the operational integrity of organizations but also to the protection of fundamental\nrights of data subjects.\nTherefore, data quality is essential for preventing data breaches, mitigating the impact of these breaches when"
  },
  {
    "chunk_id": 29,
    "text": "rights of data subjects.\nTherefore, data quality is essential for preventing data breaches, mitigating the impact of these breaches when\nthey occur, and ensuring compliance with legal obligations under the GDPR. A proactive approach to maintaining\nhigh data quality not only helps avoid data breaches but also strengthens overall compliance with the GDPR,\nprotects individuals' privacy, and upholds the integrity and reputation of organizations.\n5.Proposal method\nFollowing the rise in data privacy concerns and GDPR compliance, we propose a method for predicting personal\ndata breaches using Markov Chains. This method aims to enhance organizations' ability to anticipate and mitigate data"
  },
  {
    "chunk_id": 30,
    "text": "data breaches using Markov Chains. This method aims to enhance organizations' ability to anticipate and mitigate data\nsecurity risks.\nThe method is inspired by previous studies demonstrating the effectiveness of Markov Chains in various domains,\nincluding cyber-attack detection [9], inventory forecasting [10], air quality prediction 14], and network security\nforecasting [11]. The proposed method is divided into four stages: Data Collection and Preparation: Initially, we\n296 António Gonçalves et al. / Procedia Computer Science 256 (2025) 292–299\ngather and prepare a historical dataset documenting previous security incidents and normal network activities. These"
  },
  {
    "chunk_id": 31,
    "text": "296 António Gonçalves et al. / Procedia Computer Science 256 (2025) 292–299\ngather and prepare a historical dataset documenting previous security incidents and normal network activities. These\ndata are essential for training the Markov Chain model, and their quality is crucial for prediction accuracy.\nConstruction of the Markov Chain Model:We use the prepared data to construct a Markov Chain model, capturing\ntransitions between different data security states. The model is trained to recognize patterns that precede data breaches.\nModel Validation and Adjustment:After constructing the model, we conduct a series of tests to validate its accuracy"
  },
  {
    "chunk_id": 32,
    "text": "Model Validation and Adjustment:After constructing the model, we conduct a series of tests to validate its accuracy\nand reliability. This may involve using test datasets or simulating data breach scenarios. Based on the results, we\nadjust the model as necessary. Implementation and Monitoring: Finally, we implement the model and begin\ncontinuous monitoring. The model will predict the likelihood of future data breach occurrences, enabling security\nteams to proactively respond to mitigate potential threats.\nFor the Markov Chain model construction, it is crucial to define a set of states and transitions reflecting the various"
  },
  {
    "chunk_id": 33,
    "text": "teams to proactively respond to mitigate potential threats.\nFor the Markov Chain model construction, it is crucial to define a set of states and transitions reflecting the various\nconditions of personal data processing under GDPR principles. These conditions include accuracy, legality of\nprocessing, the right to rectification, and data protection measures described in table I. The proposed states are:\nAccurate and Updated Data (AUD): Data complying with Article 5, reflecting the required accuracy and updating.\nLegally Processed Data (LPD): Data that comply with Article 6, processed under an adequate legal basis. Data"
  },
  {
    "chunk_id": 34,
    "text": "Legally Processed Data (LPD): Data that comply with Article 6, processed under an adequate legal basis. Data\nBeing Corrected (DBC): Data being corrected by the data subject, according to Articles 15 and 16, to ensure its\naccuracy.Data with Accuracy Exception (DAE):Data that, due to specific circumstances (Article 23), may not fully\ncomply with the accuracy requirement.Protected and Minimized Data (PMD):Data that have been subject to initial\nprotection measures, such as pseudonymization and minimization, as per Article 25, to improve their quality.Data at\nRisk of Breach (DRB):Data that, despite implemented security measures (Article 32), present a high risk of breach."
  },
  {
    "chunk_id": 35,
    "text": "Risk of Breach (DRB):Data that, despite implemented security measures (Article 32), present a high risk of breach.\nThe transitions represent changes in data states resulting from internal actions of data controllers or data subjects,\nas well as external events that may influence data security. The proposed transitions are(Fig 1):From AUD to LPD:\nApplies when accurate data are subjected to legal processing.From LPD to DBC:Begins when data subjects request\ncorrection of their data.From DBC to AUD: Occurs after data correction, restoring its accuracy. From AUD/LPD\nto DAE:When exceptions to the data accuracy principleare applied.From AUD/LPD/DBC to PMD: Reflects the"
  },
  {
    "chunk_id": 36,
    "text": "to DAE:When exceptions to the data accuracy principleare applied.From AUD/LPD/DBC to PMD: Reflects the\nimplementation of data protection measures from the start.From any state to DRB:Indicates the identification of a\nhigh risk of data breach.\nThis model aims to facilitate the prediction of potential personal data breaches, underscoring the importance of\nadopting proactive and reactive measures in line with GDPR principles, particularly regarding accuracy, legality, and\ndata security.\nFigure 1Markov chain model.\nTo calculate the percentages of transitions in a Markov Chain model's matrix, it is necessary to follow a set of steps"
  },
  {
    "chunk_id": 37,
    "text": "data security.\nFigure 1Markov chain model.\nTo calculate the percentages of transitions in a Markov Chain model's matrix, it is necessary to follow a set of steps\nthat involve collecting and analysinghistorical data on the transitions between states. With this collection, it will be\npossible to calculate the transition between states. It's enough to count how many times the transition occurred in the\nhistorical data. For example, how many times the data moved from AUD (Accurate and Updated Data) to LPD\n(Legally Processed Data), from LPD to DBC (DataBeing Corrected), and so on. The transition probability from one"
  },
  {
    "chunk_id": 38,
    "text": "(Legally Processed Data), from LPD to DBC (DataBeing Corrected), and so on. The transition probability from one\nstate to another is calculated by dividing the number of times a specific transition occurred by the total number of\nexits from the initial state. The formula is:\nAntónio Gonçalves et al. / Procedia Computer Science 256 (2025) 292–299 297\n𝑁𝑁𝑃𝑃,𝑗𝑗\n𝑃𝑃𝑃𝑃,𝑗𝑗 =\nThe transition matrix P is then formed by all the transiti∑on 𝑘𝑘 pr𝑁𝑁ob𝑃𝑃a,b𝑘𝑘ilities Pij, representing the percentages of change\nfrom one state to another within the model. Each row of the matrix will sum to 100%, reflecting all possible transitions"
  },
  {
    "chunk_id": 39,
    "text": "from one state to another within the model. Each row of the matrix will sum to 100%, reflecting all possible transitions\ndeparting from a specific state.To validate the model, data must be divided into two sets: a training set and a test set.\nThe training set is used to construct the model, while the test set is used to evaluate its performance.Additionally, we\ncan utilize cross-validation, as it is a robust technique for assessing the model's generalization capability. It consists\nof dividing the data into several partitions and conducting multiple rounds of training and testing, alternating the"
  },
  {
    "chunk_id": 40,
    "text": "of dividing the data into several partitions and conducting multiple rounds of training and testing, alternating the\npartitions used for each purpose. This process helps to identify whether the model is overfitting to the training data or\nnot.\nBased on the validation results, adjustments may be necessary to improve the model's performance. This might\ninclude revising the transition probabilities, altering the defined states, or incorporating new data. Methods such as\nsensitivity analysis can be useful for determining which parameters have the greatest impact on the model's\nperformance and should therefore be adjusted.\n6.Case study"
  },
  {
    "chunk_id": 41,
    "text": "sensitivity analysis can be useful for determining which parameters have the greatest impact on the model's\nperformance and should therefore be adjusted.\n6.Case study\nToconducting a case study, an existing dataset on Kaggle [12]was utilized, which is a compilation of data from\nvarious sources detailing data breaches that occurred between 2004 and 2021, encompassing various entities and\nbreach methods. The dataset includes columns such as \"Entity\", \"Year\", \"Records\" affected, \"Type of Organization\",\nand \"Method\" of breach. The analysis focused primarily on the \"Method\" column, detailing the different ways data"
  },
  {
    "chunk_id": 42,
    "text": "and \"Method\" of breach. The analysis focused primarily on the \"Method\" column, detailing the different ways data\nwere compromised, including, but not limited tocyber-attacks (\"hacked\"), security failures (\"poor security\"), and loss\northeft of physical data carriers (\"lost/stolen media\").\nIn contextualizing the methods of data breach under GDPR, a mapping approach was adopted to align the types of\nbreaches with specific data quality attributes emphasized by the GDPR. This approach aims to understand how\ndifferent data breach incidents affect compliance and the integrity of personal data. The GDPR attributes considered"
  },
  {
    "chunk_id": 43,
    "text": "different data breach incidents affect compliance and the integrity of personal data. The GDPR attributes considered\nwere Accuracy, Purpose Limitation, Data Minimization, and Integrity and Confidentiality.\nTo represent this mapping in a Markov model structure, it would be necessary to construct a transition matrix,\nwhere each state represents a data quality attribute as defined by the GDPR. However, to build a Markov table based\non the dataset data, we need to clearly define the sequences of states (or transitions) from the data breach methods.\nNonetheless, the dataset does not provide a direct temporal sequence of transitions between breach methods for"
  },
  {
    "chunk_id": 44,
    "text": "Nonetheless, the dataset does not provide a direct temporal sequence of transitions between breach methods for\nspecific entities, which is an essential component for constructing a traditional Markov transition matrix that reflects\nthe real probabilities of transition between states.\nA possible approach is to create a table that represents the relative frequencies of the different data breach methods\npresent in the dataset. The distribution of simplified data breach methods, based on the dataset data, is presented in\nthe following table:\nTable 2. Relative frequencies of data breach.\nAttribute Frequency\nIntegrity and Confidentiality 100.00%\nAccuracy 61.02%\nLimitation 26.44%"
  },
  {
    "chunk_id": 45,
    "text": "the following table:\nTable 2. Relative frequencies of data breach.\nAttribute Frequency\nIntegrity and Confidentiality 100.00%\nAccuracy 61.02%\nLimitation 26.44%\nMinimization 20.34%\n.\n298 António Gonçalves et al. / Procedia Computer Science 256 (2025) 292–299\nHere are some observations on this distribution:Integrity and Confidentiality: This attribute is impacted by all\ncategories of data breaches, reflecting the comprehensive nature of security concerns. Therefore, it reaches a total\nfrequency of 100%, indicating that all types of data breaches contribute to the risk to this attribute.Accuracy:"
  },
  {
    "chunk_id": 46,
    "text": "frequency of 100%, indicating that all types of data breaches contribute to the risk to this attribute.Accuracy:\nPrimarily impacted by breaches involving unauthorized access or manipulation of data (such as \"Hacked\" and\n\"Accidental Exposure\"), which represent approximately 61.02% of the breaches. This underscores the importance of\nprotecting data against unauthorized changes to maintain its accuracy.Purpose Limitation and Data Minimization:\nThese attributes are affected by a smaller proportion of breaches, 26.44% and 20.34% respectively, reflecting types\nof breaches that result in the use or exposure of data beyond what is necessary for the purposes for which they were\ncollected."
  },
  {
    "chunk_id": 47,
    "text": "of breaches that result in the use or exposure of data beyond what is necessary for the purposes for which they were\ncollected.\nTo convert the distribution of data breach methods into a Markov transition matrix, we will apply a theoretical\nassumption. The assumption we will make is that, after each type of data breach, the next breach could be of any\ntype, distributed according to the relative frequencies observed in the dataset. That is, each type of breach \"transits\"\nto another type (including itself) with a probability corresponding to its relative frequency in the dataset (table 3):\nTable 3. Theoretical Markov transition matrix.\nIntegrity and Accuracy Limitation Minimization"
  },
  {
    "chunk_id": 48,
    "text": "Table 3. Theoretical Markov transition matrix.\nIntegrity and Accuracy Limitation Minimization\nConfidentiality\nIntegrity and\n48,12% 29,36% 12,72% 9,79%\nConfidentiality\nAccuracy 48,12 % 29,36% 12,72% 9,79%\nLimitation 48,12 % 29,36% 12,72% 9,79%\nMinimization 48,12 % 29,36% 12,72% 9,79%\nEach cell in this matrix indicates the transition probability from one GDPR attribute (indicated by the row) to\nanother (indicated by the column), based on the relative frequencies of data breaches affecting those attributes. For\nexample, the transition probability from \"Integrity and Confidentiality\" to \"Accuracy\" is 29.36%, reflecting the\nproportion of data breaches that impact these two attributes."
  },
  {
    "chunk_id": 49,
    "text": "example, the transition probability from \"Integrity and Confidentiality\" to \"Accuracy\" is 29.36%, reflecting the\nproportion of data breaches that impact these two attributes.\nThe theoretical Markov transition matrix we created is based on the distribution of simplified types of data\nbreaches, mapped to the data quality attributes specified by the General Data Protection Regulation (GDPR): Integrity\nand Confidentiality, Accuracy, Purpose Limitation, and Data Minimization.\nHere are the main interpretations of the matrix:Weighted Distribution of Transitions: Each row in the matrix shows"
  },
  {
    "chunk_id": 50,
    "text": "and Confidentiality, Accuracy, Purpose Limitation, and Data Minimization.\nHere are the main interpretations of the matrix:Weighted Distribution of Transitions: Each row in the matrix shows\nthe distribution of transition probabilities from one GDPR attribute to others after a data breach. For example, the high\nprobability (approximately 48.12%) of transition from any GDPR attribute back to \"Integrity and Confidentiality\"\nreflects the dominance of this theme in the dataset's data breach incidents. This suggests that, regardless of the specific\nGDPR attribute initially affected by a breach, there is a significant chance that the next breach will also affect the"
  },
  {
    "chunk_id": 51,
    "text": "GDPR attribute initially affected by a breach, there is a significant chance that the next breach will also affect the\nIntegrity and Confidentiality of the data. Widespread Impact on Data Breaches: The model suggests that breaches\naffecting Integrity and Confidentiality have the highest likelihood of occurring after any type of previous breach,\nindicating the widespread nature of data security threats across all categories. This aligns with the reality that many\ndata breaches compromise data security and protection, regardless of the initial cause.Accuracy and Data Transitions:"
  },
  {
    "chunk_id": 52,
    "text": "data breaches compromise data security and protection, regardless of the initial cause.Accuracy and Data Transitions:\nTransitions to \"Accuracy\" represent the second highest probability, indicating that many data breaches, in addition to\ncompromising security, may result in inaccurate or manipulated information. This highlights the importance of\nmeasures that ensure data accuracy following a breach, especially in contexts where data are critical for business or\npersonal decisions.Considerations on Purpose Limitation and Data Minimization: While these categories show lower\ntransition probabilities compared to \"Integrity and Confidentiality\" and \"Accuracy\", they still represent relevant post-"
  },
  {
    "chunk_id": 53,
    "text": "transition probabilities compared to \"Integrity and Confidentiality\" and \"Accuracy\", they still represent relevant post-\nbreach concerns. This underscores the importance of limiting data use to the original purposes and ensuring that only\nnecessary data are collected andretained.Strategic Implications for Data Protection: The matrix provides a theoretical\nframework for understanding how different types of breaches can affect GDPR data quality attributes. Organizations\ncan use this information to prioritize focus areas in data breach prevention and incident response planning, considering\nthe implications in terms of GDPR compliance."
  },
  {
    "chunk_id": 54,
    "text": "can use this information to prioritize focus areas in data breach prevention and incident response planning, considering\nthe implications in terms of GDPR compliance.\nAntónio Gonçalves et al. / Procedia Computer Science 256 (2025) 292–299 299\n7.Conclusion and future work\nData quality under GDPR stands as a crucial element for the efficiency and effectiveness of contemporary\ninformation systems, directly impacting organizations' ability to make informed decisions, enhance operational\nprocedures, and maintain user trust and regulatory compliance. GDPR compliance demands that personal data be"
  },
  {
    "chunk_id": 55,
    "text": "procedures, and maintain user trust and regulatory compliance. GDPR compliance demands that personal data be\naccurate, up-to-date, and retained only if necessary, highlighting the importance of stringent data management for\nprotecting privacy and personal data security.\nFuture work should focus on implementing predictive approaches, such as using Markov Chains, to anticipate and\nmitigate data security risks, enhancing organizations' ability to predict personal data breaches. This systematic and\npredictive approach allows for a detailed understanding of data quality dynamics, essential for making decisions aimed\nat accuracy, reliability, and the efficiency of data systems."
  },
  {
    "chunk_id": 56,
    "text": "predictive approach allows for a detailed understanding of data quality dynamics, essential for making decisions aimed\nat accuracy, reliability, and the efficiency of data systems.\nAdditionally, analysing the relationship between data quality and personal data breaches underscores the\nimportance of maintaining high data quality to prevent breaches, mitigate their impact when they occur, and ensure\ncompliance with GDPR legal obligations. This includes continuous data verification and updating practices, along\nwith comprehensive security measures and effective data management practices to protect against data misuse and\nprivacy violations."
  },
  {
    "chunk_id": 57,
    "text": "with comprehensive security measures and effective data management practices to protect against data misuse and\nprivacy violations.\nMoving forward, organizations are recommended to adopt a proactive approach to data quality management,\nimplementing robust data quality policies and utilizing predictive models, like Markov Chains, to anticipate potential\ndata breaches. This approach not only helps prevent data breaches but also strengthens overall GDPR compliance,\nprotects individual privacy, and upholds the integrity and reputation of organizations.\nIn summary, maintaining data quality and implementing predictive methods for managing data security risks are"
  },
  {
    "chunk_id": 58,
    "text": "protects individual privacy, and upholds the integrity and reputation of organizations.\nIn summary, maintaining data quality and implementing predictive methods for managing data security risks are\ncrucial for GDPR compliance and for protecting privacy and personal data security. Ongoing commitment to these\npractices is essential for building lasting trust relationships between organizations and data subjects, ensuring the\neffectiveness of data processing operations, and the sustainability of information systems within organizations.\nReferences\n[1] M. L. Brodie, “Data quality in information systems,” Inf. \\& Manag., vol. 3, no. 6, pp. 245–258, 1980."
  },
  {
    "chunk_id": 59,
    "text": "References\n[1] M. L. Brodie, “Data quality in information systems,” Inf. \\& Manag., vol. 3, no. 6, pp. 245–258, 1980.\n[2] T. Knauer, N. Nikiforow, and S. Wagener, “Determinants of information system quality and data quality in management\naccounting,” J. Manag. Control, vol. 31, pp. 97–121, 2020.\n[3] D. P. Ballou and H. L. Pazer, “Modeling Data and Process Quality in Multi-Input, Multi-Output Information Systems,”\nManage. Sci., vol. 31, pp. 150–162, 1985.\n[4] F. Menges et al., “Towards GDPR-compliant data processing in modern SIEM systems,” Comput. Secur., vol. 103, p.\n102165, 2021.\n[5] J. R. Norris, Markov chains, no. 2. Cambridge university press, 1998."
  },
  {
    "chunk_id": 60,
    "text": "102165, 2021.\n[5] J. R. Norris, Markov chains, no. 2. Cambridge university press, 1998.\n[6] V. Chico, “The impact of the General Data Protection Regulation on health research,” Br. Med. Bull., vol. 128, pp. 109–\n118, 2018.\n[7] K. Hjerppe, J. Ruohonen, and V. Leppänen, “The General Data Protection Regulation: Requirements, Architectures, and\nConstraints,” 2019 IEEE 27th Int. Requir. Eng. Conf., pp. 265–275, 2019.\n[8] T. Hoeren, “Big Data and Data Quality,” pp. 1–12, 2018.\n[9] N. Ye, Y. Zhang, and C. Borror, “Robustness of the Markov-chain model for cyber-attack detection,” IEEE Trans.\nReliab., vol. 53, pp. 116–123, 2004."
  },
  {
    "chunk_id": 61,
    "text": "[9] N. Ye, Y. Zhang, and C. Borror, “Robustness of the Markov-chain model for cyber-attack detection,” IEEE Trans.\nReliab., vol. 53, pp. 116–123, 2004.\n[10] Z. He and W. Jiang, “A new belief Markov chain model and its application in inventory prediction,” Int. J. Prod. Res.,\nvol. 56, pp. 2800–2817, 2017.\n[11] Y. Wang, W. Li, and Y. Liu, “A Forecast Method for Network Security Situation Based on Fuzzy Markov Chain,” pp.\n953–962, 2013.\n[12] K. Banachewicz, L. Massaron, and A. Goldbloom, The Kaggle Book: Data analysis and machine learning for\ncompetitive data science. Packt Publishing Ltd, 2022."
  }
]